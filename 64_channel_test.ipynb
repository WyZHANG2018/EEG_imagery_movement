{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reader for EDF+ files.\n",
    "TODO:\n",
    " - add support for log-transformed channels:\n",
    "   http://www.edfplus.info/specs/edffloat.html and test with\n",
    "   data generated with\n",
    "   http://www.edfplus.info/downloads/software/NeuroLoopGain.zip.\n",
    " - check annotations with Schalk's Physiobank data.\n",
    "Copyright (c) 2012 Boris Reuderink.\n",
    "'''\n",
    "\n",
    "import re, datetime, operator, logging\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "EVENT_CHANNEL = 'EDF Annotations'\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "class EDFEndOfData: pass\n",
    "\n",
    "\n",
    "def tal(tal_str):\n",
    "  '''Return a list with (onset, duration, annotation) tuples for an EDF+ TAL\n",
    "  stream.\n",
    "  '''\n",
    "  exp = '(?P<onset>[+\\-]\\d+(?:\\.\\d*)?)' + \\\n",
    "    '(?:\\x15(?P<duration>\\d+(?:\\.\\d*)?))?' + \\\n",
    "    '(\\x14(?P<annotation>[^\\x00]*))?' + \\\n",
    "    '(?:\\x14\\x00)'\n",
    "\n",
    "  def annotation_to_list(annotation):\n",
    "    return unicode(annotation, 'utf-8').split('\\x14') if annotation else []\n",
    "\n",
    "  def parse(dic):\n",
    "    return (\n",
    "      float(dic['onset']),\n",
    "      float(dic['duration']) if dic['duration'] else 0.,\n",
    "      annotation_to_list(dic['annotation']))\n",
    "\n",
    "  return [parse(m.groupdict()) for m in re.finditer(exp, tal_str)]\n",
    "\n",
    "\n",
    "def edf_header(f):\n",
    "  h = {}\n",
    "  assert f.tell() == 0  # check file position\n",
    "  assert f.read(8) == '0       '\n",
    "\n",
    "  # recording info)\n",
    "  h['local_subject_id'] = f.read(80).strip()\n",
    "  h['local_recording_id'] = f.read(80).strip()\n",
    "\n",
    "  # parse timestamp\n",
    "  (day, month, year) = [int(x) for x in re.findall('(\\d+)', f.read(8))]\n",
    "  (hour, minute, sec)= [int(x) for x in re.findall('(\\d+)', f.read(8))]\n",
    "  h['date_time'] = str(datetime.datetime(year + 2000, month, day,\n",
    "    hour, minute, sec))\n",
    "\n",
    "  # misc\n",
    "  header_nbytes = int(f.read(8))\n",
    "  subtype = f.read(44)[:5]\n",
    "  h['EDF+'] = subtype in ['EDF+C', 'EDF+D']\n",
    "  h['contiguous'] = subtype != 'EDF+D'\n",
    "  h['n_records'] = int(f.read(8))\n",
    "  h['record_length'] = float(f.read(8))  # in seconds\n",
    "  nchannels = h['n_channels'] = int(f.read(4))\n",
    "\n",
    "  # read channel info\n",
    "  channels = range(h['n_channels'])\n",
    "  h['label'] = [f.read(16).strip() for n in channels]\n",
    "  h['transducer_type'] = [f.read(80).strip() for n in channels]\n",
    "  h['units'] = [f.read(8).strip() for n in channels]\n",
    "  h['physical_min'] = np.asarray([float(f.read(8)) for n in channels])\n",
    "  h['physical_max'] = np.asarray([float(f.read(8)) for n in channels])\n",
    "  h['digital_min'] = np.asarray([float(f.read(8)) for n in channels])\n",
    "  h['digital_max'] = np.asarray([float(f.read(8)) for n in channels])\n",
    "  h['prefiltering'] = [f.read(80).strip() for n in channels]\n",
    "  h['n_samples_per_record'] = [int(f.read(8)) for n in channels]\n",
    "  f.read(32 * nchannels)  # reserved\n",
    "\n",
    "  assert f.tell() == header_nbytes\n",
    "  return h\n",
    "\n",
    "\n",
    "class BaseEDFReader:\n",
    "  def __init__(self, file):\n",
    "    self.file = file\n",
    "\n",
    "\n",
    "  def read_header(self):\n",
    "    self.header = h = edf_header(self.file)\n",
    "\n",
    "    # calculate ranges for rescaling\n",
    "    self.dig_min = h['digital_min']\n",
    "    self.phys_min = h['physical_min']\n",
    "    phys_range = h['physical_max'] - h['physical_min']\n",
    "    dig_range = h['digital_max'] - h['digital_min']\n",
    "    assert np.all(phys_range > 0)\n",
    "    assert np.all(dig_range > 0)\n",
    "    self.gain = phys_range / dig_range\n",
    "\n",
    "\n",
    "  def read_raw_record(self):\n",
    "    '''Read a record with data and return a list containing arrays with raw\n",
    "    bytes.\n",
    "    '''\n",
    "    result = []\n",
    "    for nsamp in self.header['n_samples_per_record']:\n",
    "      samples = self.file.read(nsamp * 2)\n",
    "      if len(samples) != nsamp * 2:\n",
    "        raise EDFEndOfData\n",
    "      result.append(samples)\n",
    "    return result\n",
    "\n",
    "\n",
    "  def convert_record(self, raw_record):\n",
    "    '''Convert a raw record to a (time, signals, events) tuple based on\n",
    "    information in the header.\n",
    "    '''\n",
    "    h = self.header\n",
    "    dig_min, phys_min, gain = self.dig_min, self.phys_min, self.gain\n",
    "    time = float('nan')\n",
    "    signals = []\n",
    "    events = []\n",
    "    for (i, samples) in enumerate(raw_record):\n",
    "      if h['label'][i] == EVENT_CHANNEL:\n",
    "        ann = tal(samples)\n",
    "        time = ann[0][0]\n",
    "        events.extend(ann[1:])\n",
    "        # print(i, samples)\n",
    "        # exit()\n",
    "      else:\n",
    "        # 2-byte little-endian integers\n",
    "        dig = np.fromstring(samples, '<i2').astype(np.float32)\n",
    "        phys = (dig - dig_min[i]) * gain[i] + phys_min[i]\n",
    "        signals.append(phys)\n",
    "\n",
    "    return time, signals, events\n",
    "\n",
    "\n",
    "  def read_record(self):\n",
    "    return self.convert_record(self.read_raw_record())\n",
    "\n",
    "\n",
    "  def records(self):\n",
    "    '''\n",
    "    Record generator.\n",
    "    '''\n",
    "    try:\n",
    "      while True:\n",
    "        yield self.read_record()\n",
    "    except EDFEndOfData:\n",
    "      pass\n",
    "\n",
    "\n",
    "def load_edf(edffile):\n",
    "  '''Load an EDF+ file.\n",
    "  Very basic reader for EDF and EDF+ files. While BaseEDFReader does support\n",
    "  exotic features like non-homogeneous sample rates and loading only parts of\n",
    "  the stream, load_edf expects a single fixed sample rate for all channels and\n",
    "  tries to load the whole file.\n",
    "  Parameters\n",
    "  ----------\n",
    "  edffile : file-like object or string\n",
    "  Returns\n",
    "  -------\n",
    "  Named tuple with the fields:\n",
    "    X : NumPy array with shape p by n.\n",
    "      Raw recording of n samples in p dimensions.\n",
    "    sample_rate : float\n",
    "      The sample rate of the recording. Note that mixed sample-rates are not\n",
    "      supported.\n",
    "    sens_lab : list of length p with strings\n",
    "      The labels of the sensors used to record X.\n",
    "    time : NumPy array with length n\n",
    "      The time offset in the recording for each sample.\n",
    "    annotations : a list with tuples\n",
    "      EDF+ annotations are stored in (start, duration, description) tuples.\n",
    "      start : float\n",
    "        Indicates the start of the event in seconds.\n",
    "      duration : float\n",
    "        Indicates the duration of the event in seconds.\n",
    "      description : list with strings\n",
    "        Contains (multiple?) descriptions of the annotation event.\n",
    "  '''\n",
    "  if isinstance(edffile, basestring):\n",
    "    with open(edffile, 'rb') as f:\n",
    "      return load_edf(f)  # convert filename to file\n",
    "\n",
    "  reader = BaseEDFReader(edffile)\n",
    "  reader.read_header()\n",
    "\n",
    "  h = reader.header\n",
    "  log.debug('EDF header: %s' % h)\n",
    "\n",
    "  # get sample rate info\n",
    "  nsamp = np.unique(\n",
    "    [n for (l, n) in zip(h['label'], h['n_samples_per_record'])\n",
    "    if l != EVENT_CHANNEL])\n",
    "  assert nsamp.size == 1, 'Multiple sample rates not supported!'\n",
    "  sample_rate = float(nsamp[0]) / h['record_length']\n",
    "\n",
    "  rectime, X, annotations = zip(*reader.records())\n",
    "  X = np.hstack(X)\n",
    "  annotations = reduce(operator.add, annotations)\n",
    "  chan_lab = [lab for lab in reader.header['label'] if lab != EVENT_CHANNEL]\n",
    "\n",
    "  # create timestamps\n",
    "  if reader.header['contiguous']:\n",
    "    time = np.arange(X.shape[1]) / sample_rate\n",
    "  else:\n",
    "    reclen = reader.header['record_length']\n",
    "    within_rec_time = np.linspace(0, reclen, nsamp, endpoint=False)\n",
    "    time = np.hstack([t + within_rec_time for t in rectime])\n",
    "\n",
    "  tup = namedtuple('EDF', 'X sample_rate chan_lab time annotations')\n",
    "  return tup(X, sample_rate, chan_lab, time, annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0, 22, 23, 24,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 25, 26, 27, 28, 29,  0,  0,  0],\n",
       "       [ 0, 30, 31, 32, 33, 34, 35, 36, 37, 38,  0],\n",
       "       [ 0, 39,  1,  2,  3,  4,  5,  6,  7, 40,  0],\n",
       "       [43, 41,  8,  9, 10, 11, 12, 13, 14, 42, 44],\n",
       "       [ 0, 45, 15, 16, 17, 18, 19, 20, 21, 46,  0],\n",
       "       [ 0, 47, 48, 49, 50, 51, 52, 53, 54, 55,  0],\n",
       "       [ 0,  0,  0, 56, 57, 58, 59, 60,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0, 61, 62, 63,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0, 64,  0,  0,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "channel_map=np.zeros(shape=(10,11),dtype=np.int32)\n",
    "channel_map[0,4:7]=range(22,25)\n",
    "channel_map[1,3:8]=range(25,30)\n",
    "channel_map[2,1:10]=range(30,39)\n",
    "channel_map[3,1]=39\n",
    "channel_map[3,9]=40\n",
    "channel_map[3,2:9]=range(1,8)\n",
    "channel_map[4,0]=43\n",
    "channel_map[4,1]=41\n",
    "channel_map[4,9]=42\n",
    "channel_map[4,10]=44\n",
    "channel_map[4,2:9]=range(8,15)\n",
    "channel_map[5,1]=45\n",
    "channel_map[5,9]=46\n",
    "channel_map[5,2:9]=range(15,22)\n",
    "channel_map[6,1:10]=range(47,56)\n",
    "channel_map[9,5]=64\n",
    "channel_map[8,4:7]=range(61,64)\n",
    "channel_map[7,3:8]=range(56,61)\n",
    "channel_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:130: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "win_size=10\n",
    "sample_win_matrix_0=[] #[num,time_steps,10,11]\n",
    "sample_win_matrix_1=[]\n",
    "def label(name):\n",
    "    if(name[0]=='T0'):\n",
    "        return 0\n",
    "    if((name[0]=='T1')|(name[0]=='T2')):\n",
    "        return 1\n",
    "    print name\n",
    "    print \"problem!\"\n",
    "    return 0\n",
    "    \n",
    "for person in range(1,90): \n",
    "    print person\n",
    "    if person==89: #ruined data\n",
    "        continue;\n",
    "        \n",
    "    if (person<=9):\n",
    "        index_person=\"00\"+str(person)\n",
    "    elif (person<=99):\n",
    "        index_person=\"0\"+str(person)\n",
    "\n",
    "    data=load_edf(\"../S\"+index_person+\"R01.edf\")\n",
    "    X=data[0]\n",
    "    num_wins=int((X.shape[1]-win_size)/5)+1\n",
    "    \n",
    "    num_win=0\n",
    "    while(num_win<num_wins):\n",
    "        win=X[:,num_win*5:num_win*5+win_size]\n",
    "        win_matrix=[] #np.zeros(shape=(win_size,10,11),dtype=np.float32)\n",
    "        for i in range(win_size):\n",
    "            matrix_i=np.zeros(shape=(10,11),dtype=np.float32)\n",
    "            for x in range(10):\n",
    "                for y in range(11):\n",
    "                    if (channel_map[x,y]!=0):\n",
    "                        matrix_i[x,y]=win[channel_map[x,y]-1,i]\n",
    "                    else :\n",
    "                        matrix_i[x,y]=0.0\n",
    "            win_matrix.append(matrix_i)\n",
    "        sample_win_matrix_0.append(win_matrix)\n",
    "               \n",
    "        num_win+=1\n",
    "   \n",
    "\n",
    "#     ### task 1:open and close left or right fist\n",
    "#     data=load_edf(\"../S\"+index_person+\"R03.edf\")\n",
    "#     X=data[0]\n",
    "#     time=data[3]\n",
    "#     annotation=data[4]\n",
    "#     i=0\n",
    "#     i_max=X.shape[1]\n",
    "#     for annot in annotation:\n",
    "#         if (annot[0]==0.0):\n",
    "#             while(time[i]!=0.0):\n",
    "#                 i+=1\n",
    "#         while(time[i]<annot[0]):\n",
    "#             i+=1\n",
    "#         while((time[i+win_size-1]<=annot[0]+annot[1])&(time[i+win_size-1]>annot[0])):\n",
    "#             win=X[:,i:i+win_size]\n",
    "#             win_matrix=[] \n",
    "#             for j in range(win_size):\n",
    "#                 matrix_j=np.zeros(shape=(10,11),dtype=np.float32)\n",
    "#                 for x in range(10):\n",
    "#                     for y in range(11):\n",
    "#                         if(channel_map[x,y]!=0):\n",
    "#                              matrix_j[x,y]=win[channel_map[x,y]-1,j]\n",
    "#                         else :\n",
    "#                              matrix_j[x,y]=0.0\n",
    "#                 win_matrix.append(matrix_j)\n",
    "#             sample_win_matrix_0.append(win_matrix)\n",
    "#             i+=5\n",
    "#             if(i+win_size-1>=i_max):\n",
    "#                 break\n",
    "            \n",
    "\n",
    "\n",
    "    #task 2 motion imagery \n",
    "    data=load_edf(\"../S\"+index_person+\"R04.edf\")\n",
    "    X=data[0]\n",
    "    time=data[3]\n",
    "    annotation=data[4]\n",
    "    data=load_edf(\"../S\"+index_person+\"R08.edf\")\n",
    "    X=np.concatenate((X,data[0]),axis=1)\n",
    "    time=np.concatenate((time,data[3]),axis=0)\n",
    "    annotation+=data[4]\n",
    "    data=load_edf(\"../S\"+index_person+\"R12.edf\")\n",
    "    X=np.concatenate((X,data[0]),axis=1)\n",
    "    time=np.concatenate((time,data[3]),axis=0)\n",
    "    annotation+=data[4]\n",
    "\n",
    "    i=0\n",
    "    i_max=X.shape[1]\n",
    "    for annot in annotation:\n",
    "        if (annot[0]==0.0):\n",
    "            while(time[i]!=0.0):\n",
    "                i+=1\n",
    "        while(time[i]<annot[0]):\n",
    "            i+=1\n",
    "        while((time[i+win_size-1]<=annot[0]+annot[1])&(time[i+win_size-1]>annot[0])):\n",
    "            win=X[:,i:i+win_size]\n",
    "            win_matrix=[] \n",
    "            for j in range(win_size):\n",
    "                matrix_j=np.zeros(shape=(10,11),dtype=np.float32)\n",
    "                for x in range(10):\n",
    "                    for y in range(11):\n",
    "                        if(channel_map[x,y]!=0):\n",
    "                            matrix_j[x,y]=win[channel_map[x,y]-1,j]\n",
    "                        else:\n",
    "                            matrix_j[x,y]=0.0\n",
    "                win_matrix.append(matrix_j)\n",
    "            if(label(annot[2])==0):\n",
    "                sample_win_matrix_0.append(win_matrix)\n",
    "            else :\n",
    "                sample_win_matrix_1.append(win_matrix)\n",
    "            \n",
    "            i+=5\n",
    "            if(i+win_size-1>=i_max):\n",
    "                break\n",
    "\n",
    "                \n",
    "sample_win_matrix_0=np.array(sample_win_matrix_0)\n",
    "sample_win_matrix_1=np.array(sample_win_matrix_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'filter1:0' shape=(3, 3, 1, 32) dtype=float32_ref>, <tf.Variable 'filter2:0' shape=(3, 3, 32, 64) dtype=float32_ref>, <tf.Variable 'filter3:0' shape=(3, 3, 64, 128) dtype=float32_ref>, <tf.Variable 'fn/weights:0' shape=(14080, 1024) dtype=float32_ref>, <tf.Variable 'fn/biases:0' shape=(1024,) dtype=float32_ref>, <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1088, 256) dtype=float32_ref>, <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(128, 256) dtype=float32_ref>, <tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'fully_connected/weights:0' shape=(64, 16) dtype=float32_ref>, <tf.Variable 'fully_connected/biases:0' shape=(16,) dtype=float32_ref>, <tf.Variable 'W:0' shape=(16, 2) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "#cascade models\n",
    "import tensorflow as tf\n",
    "#variables\n",
    "num_time_steps=10\n",
    "batch_size=1000\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "labels=tf.placeholder(tf.int32)\n",
    "\n",
    "\n",
    "filter1=tf.get_variable(name='filter1',shape=[3,3,1,32],dtype=tf.float32)\n",
    "filter2=tf.get_variable(name='filter2',shape=[3,3,32,64],dtype=tf.float32)\n",
    "filter3=tf.get_variable(name='filter3',shape=[3,3,64,128],dtype=tf.float32)\n",
    "with tf.variable_scope(\"fn\") as scope: \n",
    "        weights=tf.get_variable(name='weights',shape=[14080,1024],dtype=tf.float32)\n",
    "        weights=tf.get_variable(name='biases',shape=[1024],dtype=tf.float32)\n",
    "#weights_channel=tf.Variable(tf.random_uniform((1,1,1,1),minval=0.0000001,maxval=2.0,dtype=tf.float32))\n",
    "\n",
    "\n",
    "channel_matrix=[]\n",
    "for i in range(0,num_time_steps):\n",
    "    channel_matrix.append(tf.placeholder(tf.float32,shape=(1000,10,11,1)))\n",
    "cnn_output=[]\n",
    "\n",
    "for cm in channel_matrix:\n",
    "#     cm_weighted=tf.layers.conv2d(\n",
    "#         input=cm,\n",
    "#         filter=weights_channel, #[filter_height, filter_width, in_channels, out_channels]\n",
    "#         strides=[1,1,1,1],           #[1, stride, stride, 1]\n",
    "#         padding=\"SAME\",\n",
    "#         #use_cudnn_on_gpu=True,\n",
    "#         data_format='channels_last'\n",
    "#     )\n",
    "    conv1=tf.nn.conv2d(\n",
    "        input=cm, #=cm_weighted\n",
    "        filter=filter1,\n",
    "        strides=[1,1,1,1],           #[1, stride, stride, 1]\n",
    "        padding=\"SAME\",\n",
    "        use_cudnn_on_gpu=True,\n",
    "        data_format='NHWC',\n",
    "    )\n",
    "    conv1_drop=tf.nn.tanh(tf.nn.dropout(conv1,keep_prob=0.5))\n",
    "    \n",
    "    conv2=tf.nn.conv2d(\n",
    "        input=conv1_drop, #=cm_weighted\n",
    "        filter=filter2,\n",
    "        strides=[1,1,1,1],           #[1, stride, stride, 1]\n",
    "        padding=\"SAME\",\n",
    "        use_cudnn_on_gpu=True,\n",
    "        data_format='NHWC',\n",
    "    )\n",
    "    \n",
    "    conv2_drop=tf.nn.tanh(tf.nn.dropout(conv2,keep_prob=0.5))\n",
    "    \n",
    "    conv3=tf.nn.conv2d(\n",
    "        input=conv2_drop, #=cm_weighted\n",
    "        filter=filter3,\n",
    "        strides=[1,1,1,1],           #[1, stride, stride, 1]\n",
    "        padding=\"SAME\",\n",
    "        use_cudnn_on_gpu=True,\n",
    "        data_format='NHWC',\n",
    "    )\n",
    "\n",
    "    #10 11 1 \n",
    "    #f1: 8 9 32\n",
    "    #f2: 6 7 64\n",
    "    #f3: 4 5 128\n",
    "    conv3_drop=tf.nn.tanh(tf.nn.dropout(conv3,keep_prob=0.5))\n",
    "    \n",
    "    conv3_flat=tf.contrib.layers.flatten(conv3_drop)\n",
    "    \n",
    "    with tf.variable_scope(\"fn\") as scope: \n",
    "        \n",
    "        fn=tf.contrib.layers.fully_connected(\n",
    "            inputs=conv3_flat,\n",
    "            num_outputs=1024,\n",
    "            activation_fn=tf.nn.tanh,\n",
    "            normalizer_fn=None,\n",
    "            normalizer_params=None,\n",
    "            weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            weights_regularizer=None,\n",
    "            biases_initializer=tf.zeros_initializer(),\n",
    "            biases_regularizer=None,\n",
    "            reuse=True,\n",
    "            variables_collections=None,\n",
    "            outputs_collections=None,\n",
    "            trainable=True,\n",
    "            scope=scope\n",
    "        )\n",
    "    \n",
    "    \n",
    "    fn_drop=tf.nn.dropout(fn,keep_prob=0.5)\n",
    "    \n",
    "    cnn_output.append(fn_drop)\n",
    "\n",
    "\n",
    "#LSTM\n",
    "cell = tf.nn.rnn_cell.BasicLSTMCell(\n",
    "    num_units=64, \n",
    "    forget_bias=1.0,\n",
    "    activation=tf.nn.tanh,\n",
    "    state_is_tuple=True)\n",
    "\n",
    "cell_drop=tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=0.5)\n",
    "\n",
    "cell2 = tf.nn.rnn_cell.BasicLSTMCell(\n",
    "    num_units=64, \n",
    "    forget_bias=1.0, \n",
    "    activation=tf.nn.tanh,\n",
    "    state_is_tuple=True)\n",
    "\n",
    "cell2_drop=tf.contrib.rnn.DropoutWrapper(cell2, output_keep_prob=0.5)\n",
    "\n",
    "lstm_cell = tf.contrib.rnn.MultiRNNCell(cells=[cell_drop,cell2_drop], state_is_tuple=True)\n",
    "\n",
    "\n",
    "output,final_state= tf.nn.static_rnn(\n",
    "    cell=lstm_cell,\n",
    "    #cell=cell2,\n",
    "    inputs=cnn_output,\n",
    "    #A length T list of inputs,\n",
    "    #each a Tensor of shape [batch_size, input_size], \n",
    "    #or a nested tuple of such elements.\n",
    "    initial_state=None,\n",
    "    dtype=tf.float32,\n",
    "    sequence_length=tf.convert_to_tensor(np.repeat(num_time_steps,batch_size)),\n",
    "    scope=None\n",
    ")\n",
    "\n",
    "fn2=tf.contrib.layers.fully_connected(\n",
    "    inputs=output[-1],#output is just the state vectors for basiccells in tf\n",
    "    num_outputs=16,\n",
    "    activation_fn=tf.nn.tanh,\n",
    "    normalizer_fn=None,\n",
    "    normalizer_params=None,\n",
    "    weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "    weights_regularizer=None,\n",
    "    biases_initializer=tf.zeros_initializer(),\n",
    "    biases_regularizer=None,\n",
    "    reuse=None,\n",
    "    variables_collections=None,\n",
    "    outputs_collections=None,\n",
    "    trainable=True,\n",
    "    scope=None)\n",
    "\n",
    "fn2_drop=tf.nn.dropout(fn2,keep_prob=0.5)\n",
    "\n",
    "W=tf.get_variable(name='W',shape=[16,2],dtype=tf.float32)\n",
    "logits=tf.matmul(fn2_drop,W)\n",
    "crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=labels, logits=logits)\n",
    "train_loss = (tf.reduce_sum(crossent)/batch_size)\n",
    "\n",
    "# Optimization\n",
    "params = tf.trainable_variables()# the list of params to be trained\n",
    "print params #check the trainable params, including encoder's ?\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01,epsilon=0.001)\n",
    "gradients=optimizer.compute_gradients(\n",
    "    train_loss,\n",
    "    var_list=None,\n",
    "    aggregation_method=None,\n",
    "    colocate_gradients_with_ops=False,\n",
    "    grad_loss=None\n",
    ")\n",
    "\n",
    "update= optimizer.apply_gradients(gradients)\n",
    "\n",
    "\n",
    "tf.summary.scalar('LLLLoss', train_loss)\n",
    "for i,g in enumerate(gradients):\n",
    "     tf.summary.histogram('GGGGradients'+str(i), g[0])\n",
    "merged = tf.summary.merge_all()\n",
    "sess=tf.Session()\n",
    "train_writer = tf.summary.FileWriter('./summary',sess.graph)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1027914, 10, 10, 11)\n",
      "[[  0.   0.   0.   0. -49. -44. -29.   0.   0.   0.   0.]\n",
      " [  0.   0.   0. -73. -67. -41. -39. -62.   0.   0.   0.]\n",
      " [  0. -39. -42. -16. -36. -38. -46. -60. -69. -25.   0.]\n",
      " [  0. -47. -16. -29.   2.  22. -12. -23. -46. -80.   0.]\n",
      " [-52. -71. -36. -26. -18.  -4.  -4. -20. -22. -32.   1.]\n",
      " [  0. -58. -33. -39. -23. -12. -16. -27. -25. -30.   0.]\n",
      " [  0. -56. -43. -12. -18. -31. -26. -37. -33. -30.   0.]\n",
      " [  0.   0.   0. -56. -52. -35. -22. -33.   0.   0.   0.]\n",
      " [  0.   0.   0.   0. -53. -21. -11.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.  15.   0.   0.   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "print sample_win_matrix_0.shape\n",
    "print sample_win_matrix_0[0,0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess.run(init)\n",
    "\n",
    "num_0=sample_win_matrix_0.shape[0]\n",
    "num_1=sample_win_matrix_1.shape[0]\n",
    "num_window=num_0+num_1\n",
    "num_batch=int(num_window/batch_size)\n",
    "i_summary=0\n",
    "\n",
    "for i in range(num_batch):\n",
    "    \n",
    "    #sampling training \n",
    "    random_index=range(num_0)\n",
    "    np.random.shuffle(random_index)\n",
    "    batch_0=sample_win_matrix_0[random_index[:int(batch_size/2)]]\n",
    "    \n",
    "    random_index=range(num_1)\n",
    "    np.random.shuffle(random_index)\n",
    "    batch_1=sample_win_matrix_1[random_index[:int(batch_size/2)]]\n",
    "    \n",
    "    batch=np.concatenate((batch_0,batch_1),axis=0)\n",
    "    random_index=range(batch_size)\n",
    "    np.random.shuffle(random_index)\n",
    "    batch=batch[random_index]\n",
    "    batch=np.expand_dims(batch,axis=4)\n",
    "    \n",
    "    batch_labels=[]\n",
    "    for i in random_index:\n",
    "        if i < int(batch_size/2):\n",
    "            batch_labels.append(0)\n",
    "        else :\n",
    "            batch_labels.append(1)\n",
    "\n",
    "    \n",
    "    #run\n",
    "    _,summary=sess.run([update,merged],{labels:batch_labels,\n",
    "                      channel_matrix[0]:batch[:,0,:,:,:],\n",
    "                      channel_matrix[1]:batch[:,1,:,:,:], \n",
    "                      channel_matrix[2]:batch[:,2,:,:,:],\n",
    "                      channel_matrix[3]:batch[:,3,:,:,:], \n",
    "                      channel_matrix[4]:batch[:,4,:,:,:], \n",
    "                      channel_matrix[5]:batch[:,5,:,:,:], \n",
    "                      channel_matrix[6]:batch[:,6,:,:,:], \n",
    "                      channel_matrix[7]:batch[:,7,:,:,:], \n",
    "                      channel_matrix[8]:batch[:,8,:,:,:], \n",
    "                      channel_matrix[9]:batch[:,9,:,:,:]})\n",
    "    train_writer.add_summary(summary, i_summary)\n",
    "    i_summary+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03516394  0.04280772 -0.09934779 ...  0.07924265 -0.07669201\n",
      " -0.06150619]\n"
     ]
    }
   ],
   "source": [
    "print sess.run(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08952504  0.21811095]\n",
      " [ 0.10135216  0.3889083 ]\n",
      " [-0.17367351 -0.379184  ]\n",
      " ...\n",
      " [ 0.11639372 -0.37282267]\n",
      " [-0.03696988  0.02415639]\n",
      " [-0.01761211 -0.01787742]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.55      0.57      2500\n",
      "          1       0.58      0.64      0.61      2500\n",
      "\n",
      "avg / total       0.59      0.59      0.59      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "result=[]\n",
    "labels_test=[]\n",
    "for i in range(5):\n",
    "    \n",
    "    #sampling training \n",
    "    random_index=range(num_0)\n",
    "    np.random.shuffle(random_index)\n",
    "    batch_0=sample_win_matrix_0[random_index[:int(batch_size/2)]]\n",
    "    \n",
    "    random_index=range(num_1)\n",
    "    np.random.shuffle(random_index)\n",
    "    batch_1=sample_win_matrix_1[random_index[:int(batch_size/2)]]\n",
    "    \n",
    "    batch=np.concatenate((batch_0,batch_1),axis=0)\n",
    "    random_index=range(batch_size)\n",
    "    np.random.shuffle(random_index)\n",
    "    batch=batch[random_index]\n",
    "    batch=np.expand_dims(batch,axis=4)\n",
    "    \n",
    "    batch_labels=[]\n",
    "    for i in random_index:\n",
    "        if i < int(batch_size/2):\n",
    "            batch_labels.append(0)\n",
    "        else :\n",
    "            batch_labels.append(1)\n",
    "    labels_test+=batch_labels\n",
    "    #run \n",
    "    r=sess.run([logits],{channel_matrix[0]:batch[:,0,:,:,:],\n",
    "                      channel_matrix[1]:batch[:,1,:,:,:], \n",
    "                      channel_matrix[2]:batch[:,2,:,:,:],\n",
    "                      channel_matrix[3]:batch[:,3,:,:,:], \n",
    "                      channel_matrix[4]:batch[:,4,:,:,:], \n",
    "                      channel_matrix[5]:batch[:,5,:,:,:], \n",
    "                      channel_matrix[6]:batch[:,6,:,:,:], \n",
    "                      channel_matrix[7]:batch[:,7,:,:,:], \n",
    "                      channel_matrix[8]:batch[:,8,:,:,:], \n",
    "                      channel_matrix[9]:batch[:,9,:,:,:]})\n",
    "    result+=r\n",
    "print result[0]\n",
    "    \n",
    "result=np.concatenate(result,axis=0)\n",
    "result=np.array(result)\n",
    "result=np.argmax(result,axis=1)\n",
    "labels_test=np.array(labels_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print classification_report(labels_test,result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_total=[]\n",
    "y_total=[]\n",
    "channel_available=[60,62,31,35,48,52,8,12]\n",
    "for person in range(1,93): \n",
    "    if person==89: #ruined data\n",
    "        continue;\n",
    "        \n",
    "    if (person<=9):\n",
    "        index_person=\"00\"+str(person)\n",
    "    elif (person<=99):\n",
    "        index_person=\"0\"+str(person)\n",
    "        \n",
    "    #baseline\n",
    "    data=load_edf(\"../S\"+index_person+\"R01.edf\")\n",
    "    X=data[0][channel_available]\n",
    "    num_wins=int((X.shape[1]-win_size)/5)+1\n",
    "    \n",
    "    num_win=0\n",
    "    while(num_win<num_wins):\n",
    "        X_total.append(np.ndarray.flatten(X[:,num_win*5:num_win*5+win_size]))     \n",
    "        y_total.append(0)\n",
    "               \n",
    "        num_win+=1\n",
    "   \n",
    "\n",
    "    ### task 1:open and close left or right fist\n",
    "    data=load_edf(\"../S\"+index_person+\"R03.edf\")\n",
    "    X=data[0][channel_available]\n",
    "    time=data[3]\n",
    "    annotation=data[4]\n",
    "    i=0\n",
    "    i_max=X.shape[1]\n",
    "    for annot in annotation:\n",
    "        if (annot[0]==0.0):\n",
    "            while(time[i]!=0.0):\n",
    "                i+=1\n",
    "        while(time[i]<annot[0]):\n",
    "            i+=1\n",
    "        while((time[i+win_size-1]<=annot[0]+annot[1])&(time[i+win_size-1]>annot[0])):\n",
    "            X_total.append(np.ndarray.flatten(X[:,i:i+win_size]))\n",
    "            y_total.append(0)\n",
    "            i+=5\n",
    "            if(i+win_size-1>=i_max):\n",
    "                break\n",
    "            \n",
    "\n",
    "\n",
    "    #task 2 motion imagery \n",
    "    data=load_edf(\"../S\"+index_person+\"R04.edf\")\n",
    "    X=data[0][channel_available]\n",
    "    time=data[3]\n",
    "    annotation=data[4]\n",
    "    data=load_edf(\"../S\"+index_person+\"R08.edf\")\n",
    "    X=np.concatenate((X,data[0][channel_available]),axis=1)\n",
    "    time=np.concatenate((time,data[3]),axis=0)\n",
    "    annotation+=data[4]\n",
    "    data=load_edf(\"../S\"+index_person+\"R12.edf\")\n",
    "    X=np.concatenate((X,data[0][channel_available]),axis=1)\n",
    "    time=np.concatenate((time,data[3]),axis=0)\n",
    "    annotation+=data[4]\n",
    "\n",
    "    i=0\n",
    "    i_max=X.shape[1]\n",
    "    for annot in annotation:\n",
    "        if (annot[0]==0.0):\n",
    "            while(time[i]!=0.0):\n",
    "                i+=1\n",
    "        while(time[i]<annot[0]):\n",
    "            i+=1\n",
    "        while((time[i+win_size-1]<=annot[0]+annot[1])&(time[i+win_size-1]>annot[0])):\n",
    "            X_total.append(np.ndarray.flatten(X[:,i:i+win_size]))       \n",
    "            y_total.append(label(annot[2]))\n",
    "            \n",
    "            i+=5\n",
    "            if(i+win_size-1>=i_max):\n",
    "                break\n",
    "\n",
    "#shuffle the data \n",
    "X_total=np.array(X_total)\n",
    "y_total=np.array(y_total)\n",
    "num_window=X_total.shape[0]\n",
    "random_index=range(num_window)\n",
    "np.random.shuffle(random_index)\n",
    "X_total=X_total[random_index]\n",
    "y_total=y_total[random_index]\n",
    "X_train=X_total[:int(num_window/3*2),:]\n",
    "y_train=y_total[:int(num_window/3*2)]\n",
    "X_test=X_total[int(num_window/3*2):,:]\n",
    "y_test=y_total[int(num_window/3*2):]\n",
    "\n",
    "\n",
    "#LDA+ML\n",
    "#is there online training in sklearn ? =.=\n",
    "from sklearn.decomposition import IncrementalPCA as PCA\n",
    "from sklearn.linear_model import SGDClassifier as SGD\n",
    "#dim initial= 10*8=80\n",
    "pca=PCA(n_components=10,batch_size=100)\n",
    "pca.fit(X_train)\n",
    "x_train=pca.transform(X_train)\n",
    "\n",
    "sgd=SGD()\n",
    "num_window=X_train.shape[0]\n",
    "num_batch=int(num_window/100)-1\n",
    "for i in range(num_batch):\n",
    "    X_batch=x_train[i*100:(i+1)*100,:]\n",
    "    y_batch=y_train[i*100:(i+1)*100]\n",
    "    sgd=sgd.partial_fit(x_batch,y_batch)\n",
    "\n",
    "x_test=pca.transform(X_test)\n",
    "result=sgd.predict(x_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print classification_report(np.array(y_test),np.array(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Variable:0' shape=(1, 2, 1, 32) dtype=float32_ref>, <tf.Variable 'Variable_1:0' shape=(1, 2, 32, 16) dtype=float32_ref>, <tf.Variable 'Variable_2:0' shape=(8, 8, 16, 8) dtype=float32_ref>, <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(24, 64) dtype=float32_ref>, <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(24, 32) dtype=float32_ref>, <tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'Variable_3:0' shape=(8, 2) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#variables\n",
    "tf.reset_default_graph()\n",
    "batch_size=222\n",
    "num_channels=8\n",
    "num_frequency=22\n",
    "\n",
    "channel_frequency_input=tf.placeholder(tf.float32,shape=(222,num_channels,num_frequency,1)) #[batch, in_height=channels, in_width=frequency, in_channels=1]\n",
    "encoder_labels=tf.placeholder(tf.int32)\n",
    "# filter1=tf.Variable(tf.random_uniform((2,3,1,32),minval=0.0000001,maxval=0.00001,dtype=tf.float32))\n",
    "# filter2=tf.Variable(tf.random_uniform((2,3,32,16),minval=0.0000001,maxval=0.00001,dtype=tf.float32))\n",
    "# filter3=tf.Variable(tf.random_uniform((4,6,16,8),minval=0.0000001,maxval=0.00001,dtype=tf.float32))\n",
    "filter1=tf.Variable(tf.random_uniform((1,2,1,32),minval=0.0000001,maxval=0.00001,dtype=tf.float32))\n",
    "filter2=tf.Variable(tf.random_uniform((1,2,32,16),minval=0.0000001,maxval=0.00001,dtype=tf.float32))\n",
    "filter3=tf.Variable(tf.random_uniform((8,8,16,8),minval=0.0000001,maxval=0.00001,dtype=tf.float32))\n",
    "# filter1=tf.Variable(tf.random_uniform((3,3,1,16),minval=0.0000001,maxval=0.00001,dtype=tf.float32))\n",
    "# filter2=tf.Variable(tf.random_uniform((4,8,16,8),minval=0.0000001,maxval=0.00001,dtype=tf.float32))\n",
    "\n",
    "conv1=tf.nn.conv2d(\n",
    "    input=channel_frequency_input,\n",
    "    filter=filter1, #[filter_height, filter_width, in_channels, out_channels]\n",
    "    strides=[1,1,2,1],           #[1, stride, stride, 1]\n",
    "    padding=\"VALID\",\n",
    "    #use_cudnn_on_gpu=True,\n",
    "    data_format='NHWC'\n",
    ")\n",
    "\n",
    "pool1=tf.nn.max_pool(\n",
    "    value=conv1,\n",
    "    ksize=[1,1,2,1],\n",
    "    strides=[1,1,1,1],\n",
    "    padding='VALID',\n",
    "    data_format='NHWC'\n",
    ")\n",
    "\n",
    "conv2=tf.nn.conv2d(\n",
    "    input=pool1,\n",
    "    filter=filter2, #[filter_height, filter_width, in_channels, out_channels]\n",
    "    strides=[1,1,1,1],           #[1, stride, stride, 1]\n",
    "    padding=\"VALID\",\n",
    "    #use_cudnn_on_gpu=True,\n",
    "    data_format='NHWC'\n",
    ")\n",
    "\n",
    "pool2=tf.nn.max_pool(\n",
    "    value=conv2,\n",
    "    ksize=[1,1,2,1],\n",
    "    strides=[1,1,1,1],\n",
    "    padding='VALID',\n",
    "    data_format='NHWC'\n",
    ")\n",
    "\n",
    "conv3=tf.nn.conv2d(\n",
    "    input=pool2,\n",
    "    filter=filter3, #[filter_height, filter_width, in_channels, out_channels]\n",
    "    strides=[1,1,1,1],           #[1, stride, stride, 1]\n",
    "    padding=\"VALID\",\n",
    "    #use_cudnn_on_gpu=True,\n",
    "    data_format='NHWC'\n",
    ")\n",
    "\n",
    "flat_cnn=tf.contrib.layers.flatten(conv3)\n",
    "#flat_cnn=tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "\n",
    "output_cnn=tf.sigmoid(flat_cnn)\n",
    "output_shape1=tf.size(output_cnn)\n",
    "\n",
    "#LSTM\n",
    "num_time_steps=20\n",
    "channel_amplitude_input=[]\n",
    "for i in range(0,num_time_steps):\n",
    "    channel_amplitude_input.append(tf.placeholder(tf.float32,shape=(222,num_channels)))\n",
    "\n",
    "\n",
    "cell = tf.nn.rnn_cell.BasicLSTMCell(\n",
    "    num_units=16, \n",
    "    forget_bias=1.0, \n",
    "    state_is_tuple=True)\n",
    "\n",
    "cell2 = tf.nn.rnn_cell.BasicLSTMCell(\n",
    "    num_units=8, \n",
    "    forget_bias=1.0, \n",
    "    state_is_tuple=True)\n",
    "\n",
    "lstm_cell = tf.contrib.rnn.MultiRNNCell(cells=[cell,cell2], state_is_tuple=True)\n",
    "\n",
    "\n",
    "output,final_state= tf.nn.static_rnn(\n",
    "    cell=lstm_cell,\n",
    "    #cell=cell2,\n",
    "    inputs=channel_amplitude_input,\n",
    "    #A length T list of inputs,\n",
    "    #each a Tensor of shape [batch_size, input_size], \n",
    "    #or a nested tuple of such elements.\n",
    "    initial_state=None,\n",
    "    dtype=tf.float32,\n",
    "    sequence_length=tf.convert_to_tensor(np.repeat(num_time_steps,batch_size)),\n",
    "    scope=None\n",
    ")\n",
    "\n",
    "output_shape2=tf.size(output[-1])\n",
    "flat_lstm=tf.contrib.layers.flatten(output[-1])\n",
    "\n",
    "#vec_concat=tf.concat([output_cnn,flat_lstm],axis=1)\n",
    "\n",
    "\n",
    "W=tf.Variable(tf.random_uniform((8,2),minval=0.0000001,maxval=0.00001,dtype=tf.float32)) \n",
    "logits=tf.matmul(flat_lstm,W)\n",
    "crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=encoder_labels, logits=logits)\n",
    "train_loss = (tf.reduce_sum(crossent)/batch_size)\n",
    "\n",
    "# Optimization\n",
    "params = tf.trainable_variables()# the list of params to be trained\n",
    "print params #check the trainable params, including encoder's ?\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001,epsilon=0.001)\n",
    "gradients=optimizer.compute_gradients(\n",
    "    train_loss,\n",
    "    var_list=None,\n",
    "    aggregation_method=None,\n",
    "    colocate_gradients_with_ops=False,\n",
    "    grad_loss=None\n",
    ")\n",
    "\n",
    "update= optimizer.apply_gradients(gradients)\n",
    "\n",
    "\n",
    "tf.summary.scalar('LLLLoss', train_loss)\n",
    "# for i,g in enumerate(gradients):\n",
    "#     tf.summary.histogram('GGGGradients'+str(i), g[0])\n",
    "merged = tf.summary.merge_all()\n",
    "sess=tf.Session()\n",
    "train_writer = tf.summary.FileWriter('./summary',sess.graph)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# channel_amp=[]\n",
    "# for i in range(10):\n",
    "#     channel_amp.append(np.zeros(shape=(222,8),dtype=\"float32\"))\n",
    "\n",
    "# g,s,s2,l,_,summ=sess.run([gradients,output_shape1,output_shape2,train_loss,update,merged],{channel_frequency_input:np.zeros(shape=(222,8,12,1),dtype=\"float32\"),\\\n",
    "#                           encoder_labels:np.repeat(1,222),\n",
    "#                           channel_amplitude_input[0]:channel_amp[0],  \n",
    "#                           channel_amplitude_input[1]:channel_amp[1], \n",
    "#                           channel_amplitude_input[2]:channel_amp[2],\n",
    "#                           channel_amplitude_input[3]:channel_amp[3], \n",
    "#                           channel_amplitude_input[4]:channel_amp[4], \n",
    "#                           channel_amplitude_input[5]:channel_amp[5], \n",
    "#                           channel_amplitude_input[6]:channel_amp[6], \n",
    "#                           channel_amplitude_input[7]:channel_amp[7], \n",
    "#                           channel_amplitude_input[8]:channel_amp[8], \n",
    "#                           channel_amplitude_input[9]:channel_amp[9]})\n",
    "# writer.add_summary(summ, 1)\n",
    "# writer.add_summary(summ, 2)\n",
    "# print type(g)\n",
    "# print g[0]\n",
    "# print s\n",
    "# print s2\n",
    "# print l"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
