{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reader for EDF+ files.\n",
    "TODO:\n",
    " - add support for log-transformed channels:\n",
    "   http://www.edfplus.info/specs/edffloat.html and test with\n",
    "   data generated with\n",
    "   http://www.edfplus.info/downloads/software/NeuroLoopGain.zip.\n",
    " - check annotations with Schalk's Physiobank data.\n",
    "Copyright (c) 2012 Boris Reuderink.\n",
    "'''\n",
    "\n",
    "import re, datetime, operator, logging\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "EVENT_CHANNEL = 'EDF Annotations'\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "class EDFEndOfData: pass\n",
    "\n",
    "\n",
    "def tal(tal_str):\n",
    "  '''Return a list with (onset, duration, annotation) tuples for an EDF+ TAL\n",
    "  stream.\n",
    "  '''\n",
    "  exp = '(?P<onset>[+\\-]\\d+(?:\\.\\d*)?)' + \\\n",
    "    '(?:\\x15(?P<duration>\\d+(?:\\.\\d*)?))?' + \\\n",
    "    '(\\x14(?P<annotation>[^\\x00]*))?' + \\\n",
    "    '(?:\\x14\\x00)'\n",
    "\n",
    "  def annotation_to_list(annotation):\n",
    "    return unicode(annotation, 'utf-8').split('\\x14') if annotation else []\n",
    "\n",
    "  def parse(dic):\n",
    "    return (\n",
    "      float(dic['onset']),\n",
    "      float(dic['duration']) if dic['duration'] else 0.,\n",
    "      annotation_to_list(dic['annotation']))\n",
    "\n",
    "  return [parse(m.groupdict()) for m in re.finditer(exp, tal_str)]\n",
    "\n",
    "\n",
    "def edf_header(f):\n",
    "  h = {}\n",
    "  assert f.tell() == 0  # check file position\n",
    "  assert f.read(8) == '0       '\n",
    "\n",
    "  # recording info)\n",
    "  h['local_subject_id'] = f.read(80).strip()\n",
    "  h['local_recording_id'] = f.read(80).strip()\n",
    "\n",
    "  # parse timestamp\n",
    "  (day, month, year) = [int(x) for x in re.findall('(\\d+)', f.read(8))]\n",
    "  (hour, minute, sec)= [int(x) for x in re.findall('(\\d+)', f.read(8))]\n",
    "  h['date_time'] = str(datetime.datetime(year + 2000, month, day,\n",
    "    hour, minute, sec))\n",
    "\n",
    "  # misc\n",
    "  header_nbytes = int(f.read(8))\n",
    "  subtype = f.read(44)[:5]\n",
    "  h['EDF+'] = subtype in ['EDF+C', 'EDF+D']\n",
    "  h['contiguous'] = subtype != 'EDF+D'\n",
    "  h['n_records'] = int(f.read(8))\n",
    "  h['record_length'] = float(f.read(8))  # in seconds\n",
    "  nchannels = h['n_channels'] = int(f.read(4))\n",
    "\n",
    "  # read channel info\n",
    "  channels = range(h['n_channels'])\n",
    "  h['label'] = [f.read(16).strip() for n in channels]\n",
    "  h['transducer_type'] = [f.read(80).strip() for n in channels]\n",
    "  h['units'] = [f.read(8).strip() for n in channels]\n",
    "  h['physical_min'] = np.asarray([float(f.read(8)) for n in channels])\n",
    "  h['physical_max'] = np.asarray([float(f.read(8)) for n in channels])\n",
    "  h['digital_min'] = np.asarray([float(f.read(8)) for n in channels])\n",
    "  h['digital_max'] = np.asarray([float(f.read(8)) for n in channels])\n",
    "  h['prefiltering'] = [f.read(80).strip() for n in channels]\n",
    "  h['n_samples_per_record'] = [int(f.read(8)) for n in channels]\n",
    "  f.read(32 * nchannels)  # reserved\n",
    "\n",
    "  assert f.tell() == header_nbytes\n",
    "  return h\n",
    "\n",
    "\n",
    "class BaseEDFReader:\n",
    "  def __init__(self, file):\n",
    "    self.file = file\n",
    "\n",
    "\n",
    "  def read_header(self):\n",
    "    self.header = h = edf_header(self.file)\n",
    "\n",
    "    # calculate ranges for rescaling\n",
    "    self.dig_min = h['digital_min']\n",
    "    self.phys_min = h['physical_min']\n",
    "    phys_range = h['physical_max'] - h['physical_min']\n",
    "    dig_range = h['digital_max'] - h['digital_min']\n",
    "    assert np.all(phys_range > 0)\n",
    "    assert np.all(dig_range > 0)\n",
    "    self.gain = phys_range / dig_range\n",
    "\n",
    "\n",
    "  def read_raw_record(self):\n",
    "    '''Read a record with data and return a list containing arrays with raw\n",
    "    bytes.\n",
    "    '''\n",
    "    result = []\n",
    "    for nsamp in self.header['n_samples_per_record']:\n",
    "      samples = self.file.read(nsamp * 2)\n",
    "      if len(samples) != nsamp * 2:\n",
    "        raise EDFEndOfData\n",
    "      result.append(samples)\n",
    "    return result\n",
    "\n",
    "\n",
    "  def convert_record(self, raw_record):\n",
    "    '''Convert a raw record to a (time, signals, events) tuple based on\n",
    "    information in the header.\n",
    "    '''\n",
    "    h = self.header\n",
    "    dig_min, phys_min, gain = self.dig_min, self.phys_min, self.gain\n",
    "    time = float('nan')\n",
    "    signals = []\n",
    "    events = []\n",
    "    for (i, samples) in enumerate(raw_record):\n",
    "      if h['label'][i] == EVENT_CHANNEL:\n",
    "        ann = tal(samples)\n",
    "        time = ann[0][0]\n",
    "        events.extend(ann[1:])\n",
    "        # print(i, samples)\n",
    "        # exit()\n",
    "      else:\n",
    "        # 2-byte little-endian integers\n",
    "        dig = np.fromstring(samples, '<i2').astype(np.float32)\n",
    "        phys = (dig - dig_min[i]) * gain[i] + phys_min[i]\n",
    "        signals.append(phys)\n",
    "\n",
    "    return time, signals, events\n",
    "\n",
    "\n",
    "  def read_record(self):\n",
    "    return self.convert_record(self.read_raw_record())\n",
    "\n",
    "\n",
    "  def records(self):\n",
    "    '''\n",
    "    Record generator.\n",
    "    '''\n",
    "    try:\n",
    "      while True:\n",
    "        yield self.read_record()\n",
    "    except EDFEndOfData:\n",
    "      pass\n",
    "\n",
    "\n",
    "def load_edf(edffile):\n",
    "  '''Load an EDF+ file.\n",
    "  Very basic reader for EDF and EDF+ files. While BaseEDFReader does support\n",
    "  exotic features like non-homogeneous sample rates and loading only parts of\n",
    "  the stream, load_edf expects a single fixed sample rate for all channels and\n",
    "  tries to load the whole file.\n",
    "  Parameters\n",
    "  ----------\n",
    "  edffile : file-like object or string\n",
    "  Returns\n",
    "  -------\n",
    "  Named tuple with the fields:\n",
    "    X : NumPy array with shape p by n.\n",
    "      Raw recording of n samples in p dimensions.\n",
    "    sample_rate : float\n",
    "      The sample rate of the recording. Note that mixed sample-rates are not\n",
    "      supported.\n",
    "    sens_lab : list of length p with strings\n",
    "      The labels of the sensors used to record X.\n",
    "    time : NumPy array with length n\n",
    "      The time offset in the recording for each sample.\n",
    "    annotations : a list with tuples\n",
    "      EDF+ annotations are stored in (start, duration, description) tuples.\n",
    "      start : float\n",
    "        Indicates the start of the event in seconds.\n",
    "      duration : float\n",
    "        Indicates the duration of the event in seconds.\n",
    "      description : list with strings\n",
    "        Contains (multiple?) descriptions of the annotation event.\n",
    "  '''\n",
    "  if isinstance(edffile, basestring):\n",
    "    with open(edffile, 'rb') as f:\n",
    "      return load_edf(f)  # convert filename to file\n",
    "\n",
    "  reader = BaseEDFReader(edffile)\n",
    "  reader.read_header()\n",
    "\n",
    "  h = reader.header\n",
    "  log.debug('EDF header: %s' % h)\n",
    "\n",
    "  # get sample rate info\n",
    "  nsamp = np.unique(\n",
    "    [n for (l, n) in zip(h['label'], h['n_samples_per_record'])\n",
    "    if l != EVENT_CHANNEL])\n",
    "  assert nsamp.size == 1, 'Multiple sample rates not supported!'\n",
    "  sample_rate = float(nsamp[0]) / h['record_length']\n",
    "\n",
    "  rectime, X, annotations = zip(*reader.records())\n",
    "  X = np.hstack(X)\n",
    "  annotations = reduce(operator.add, annotations)\n",
    "  chan_lab = [lab for lab in reader.header['label'] if lab != EVENT_CHANNEL]\n",
    "\n",
    "  # create timestamps\n",
    "  if reader.header['contiguous']:\n",
    "    time = np.arange(X.shape[1]) / sample_rate\n",
    "  else:\n",
    "    reclen = reader.header['record_length']\n",
    "    within_rec_time = np.linspace(0, reclen, nsamp, endpoint=False)\n",
    "    time = np.hstack([t + within_rec_time for t in rectime])\n",
    "\n",
    "  tup = namedtuple('EDF', 'X sample_rate chan_lab time annotations')\n",
    "  return tup(X, sample_rate, chan_lab, time, annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "#convolution only within the dimension of timesteps (not with the channels)\n",
    "#such method may work as a smoother...\n",
    "\n",
    "batch_size=1000\n",
    "num_time_steps=20\n",
    "labels=tf.placeholder(tf.int32,shape=[1000])\n",
    "inputs=tf.placeholder(tf.float32,shape=(1000,8,num_time_steps,1))\n",
    "#with tf.name_scope(\"main/\"):\n",
    "\n",
    "#8*20*1\n",
    "filter1=tf.get_variable(name='filter1',shape=[1,3,1,8],dtype=tf.float32)\n",
    "#8*18*8\n",
    "filter2=tf.get_variable(name='filter2',shape=[1,3,8,8],dtype=tf.float32)\n",
    "#8*16*8\n",
    "\n",
    "conv1=tf.nn.conv2d(\n",
    "    input=inputs, #=cm_weighted\n",
    "    filter=filter1,\n",
    "    strides=[1,1,1,1], #[1, stride, stride, 1]\n",
    "    padding=\"VALID\",\n",
    "    use_cudnn_on_gpu=True,\n",
    "    data_format='NHWC',\n",
    ")\n",
    "\n",
    "conv1_tanh=tf.nn.tanh(conv1)\n",
    "\n",
    "conv2=tf.nn.conv2d(\n",
    "    input=conv1_tanh, #=cm_weighted\n",
    "    filter=filter2,\n",
    "    strides=[1,1,1,1], #[1, stride, stride, 1]\n",
    "    padding=\"VALID\",\n",
    "    use_cudnn_on_gpu=True,\n",
    "    data_format='NHWC',\n",
    ")\n",
    "\n",
    "conv2_tanh=tf.nn.tanh(conv2)\n",
    "conv2_flat=tf.contrib.layers.flatten(conv2_tanh)\n",
    "\n",
    "fn1=tf.contrib.layers.fully_connected(\n",
    "    inputs=conv2_flat,\n",
    "    num_outputs=20,\n",
    "    activation_fn=tf.nn.tanh,\n",
    "    normalizer_fn=None,\n",
    "    normalizer_params=None,\n",
    "    weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "    weights_regularizer=None,\n",
    "    biases_initializer=tf.zeros_initializer(),\n",
    "    biases_regularizer=None,\n",
    "    reuse=None,\n",
    "    variables_collections=None,\n",
    "    outputs_collections=None,\n",
    "    trainable=True,\n",
    "    scope=None\n",
    ")\n",
    "\n",
    "#at this level, we add a lda like a regularisation\n",
    "#LDAloss=(moy-moy)^2/(var+var)\n",
    "\n",
    "#with tf.name_scope(\"where\"):\n",
    "condition1=tf.cast(labels, tf.bool)\n",
    "condition0=tf.logical_not(condition1)\n",
    "pos1=tf.where(condition1)\n",
    "pos0=tf.where(condition0)\n",
    "input_1=tf.gather(params=fn1,indices=pos1)\n",
    "input_0=tf.gather(params=fn1,indices=pos0)\n",
    "\n",
    "mean1,var1=tf.nn.moments(input_1,axes=0)\n",
    "mean0,var0=tf.nn.moments(input_0,axes=0)\n",
    "\n",
    "ldaloss=-tf.square(tf.norm(mean1-mean0))/(tf.keras.backend.sum(var1)+tf.keras.backend.sum(var0))\n",
    "\n",
    "\n",
    "#here comes the classifier\n",
    "\n",
    "fn2=tf.contrib.layers.fully_connected(\n",
    "    inputs=fn1,\n",
    "    num_outputs=10,\n",
    "    activation_fn=tf.nn.tanh,\n",
    "    normalizer_fn=None,\n",
    "    normalizer_params=None,\n",
    "    weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "    weights_regularizer=None,\n",
    "    biases_initializer=tf.zeros_initializer(),\n",
    "    biases_regularizer=None,\n",
    "    reuse=None,\n",
    "    variables_collections=None,\n",
    "    outputs_collections=None,\n",
    "    trainable=True,\n",
    "    scope=None\n",
    ")\n",
    "\n",
    "fn3=tf.contrib.layers.fully_connected(\n",
    "    inputs=fn2,\n",
    "    num_outputs=2,\n",
    "    activation_fn=tf.nn.tanh,\n",
    "    normalizer_fn=None,\n",
    "    normalizer_params=None,\n",
    "    weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "    weights_regularizer=None,\n",
    "    biases_initializer=tf.zeros_initializer(),\n",
    "    biases_regularizer=None,\n",
    "    reuse=None,\n",
    "    variables_collections=None,\n",
    "    outputs_collections=None,\n",
    "    trainable=True,\n",
    "    scope=None\n",
    ")\n",
    "\n",
    "#loss and train\n",
    "crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=labels, logits=fn3)\n",
    "#training loss with lda regularisation\n",
    "train_loss = (tf.reduce_sum(crossent)/batch_size+100*ldaloss)\n",
    "\n",
    "# Optimization\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001,epsilon=0.001)\n",
    "gradients=optimizer.compute_gradients(\n",
    "    train_loss,\n",
    "    var_list=None,\n",
    "    aggregation_method=None,\n",
    "    colocate_gradients_with_ops=False,\n",
    "    grad_loss=None\n",
    ")#return A list of (gradient, variable) pairs\n",
    "update= optimizer.apply_gradients(gradients)\n",
    "\n",
    "tf.summary.scalar('Loss_entropy', train_loss)\n",
    "tf.summary.scalar('Loss_lda', ldaloss)\n",
    "for i,g in enumerate(gradients):\n",
    "    tf.summary.histogram('GGGGradients'+str(i), tf.convert_to_tensor(g[0]))\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "sess=tf.Session()\n",
    "train_writer = tf.summary.FileWriter('./summary',sess.graph)\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "    \n",
    "#debug which grandient is none\n",
    "#     for i,g in enumerate(gradients):\n",
    "#         print sess.run(g[1])\n",
    "#         if (g[0]==None):\n",
    "#             print tf.trainable_variables()[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:130: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n"
     ]
    }
   ],
   "source": [
    "channel_available=[60,62,31,35,48,52,8,12]\n",
    "win_size=num_time_steps\n",
    "sample_matrix_0=[] #[batch_size,channels,num_time_steps]\n",
    "sample_matrix_1=[]\n",
    "i_summary=0\n",
    "def label(name):\n",
    "    if(name[0]=='T0'):\n",
    "        return 0\n",
    "    if((name[0]=='T1')|(name[0]=='T2')):\n",
    "        return 1\n",
    "    print name\n",
    "    print \"problem!\"\n",
    "    return 0\n",
    "    \n",
    "for person in range(1,109): \n",
    "    print person\n",
    "    if person==89: #ruined data\n",
    "        continue;\n",
    "        \n",
    "    if (person<=9):\n",
    "        index_person=\"00\"+str(person)\n",
    "    elif (person<=99):\n",
    "        index_person=\"0\"+str(person)\n",
    "\n",
    "    ### baseline\n",
    "    data=load_edf(\"../S\"+index_person+\"R01.edf\")\n",
    "    X=data[0][channel_available]\n",
    "    num_wins=int((X.shape[1]-win_size)/5)+1\n",
    "    \n",
    "    num_win=0\n",
    "    while(num_win<num_wins):  \n",
    "        sample_matrix_0.append(X[:,num_win*5:num_win*5+win_size])               \n",
    "        num_win+=1\n",
    "   \n",
    "\n",
    "     ### task 1:open and close left or right fist\n",
    "#     data=load_edf(\"../S\"+index_person+\"R03.edf\")\n",
    "#     X=data[0][channel_available]\n",
    "#     time=data[3]\n",
    "#     annotation=data[4]\n",
    "    \n",
    "#     i=0\n",
    "#     i_max=X.shape[1]\n",
    "#     for annot in annotation:\n",
    "#         if (annot[0]==0.0):\n",
    "#             while(time[i]!=0.0):\n",
    "#                 i+=1\n",
    "#         while(time[i]<annot[0]):\n",
    "#             i+=1\n",
    "#         while((time[i+win_size-1]<=annot[0]+annot[1])&(time[i+win_size-1]>annot[0])):\n",
    "#             sample_matrix_0.append(X[:,i:i+win_size])\n",
    "#             i+=5\n",
    "#             if(i+win_size-1>=i_max):\n",
    "#                 break\n",
    "            \n",
    "\n",
    "\n",
    "    #task 2 motion imagery \n",
    "    data=load_edf(\"../S\"+index_person+\"R04.edf\")\n",
    "    X=data[0][channel_available]\n",
    "    time=data[3]\n",
    "    annotation=data[4]\n",
    "    data=load_edf(\"../S\"+index_person+\"R08.edf\")\n",
    "    X=np.concatenate((X,data[0][channel_available]),axis=1)\n",
    "    time=np.concatenate((time,data[3]),axis=0)\n",
    "    annotation+=data[4]\n",
    "    data=load_edf(\"../S\"+index_person+\"R12.edf\")\n",
    "    X=np.concatenate((X,data[0][channel_available]),axis=1)\n",
    "    time=np.concatenate((time,data[3]),axis=0)\n",
    "    annotation+=data[4]\n",
    "\n",
    "    i=0\n",
    "    i_max=X.shape[1]\n",
    "    for annot in annotation:\n",
    "        if (annot[0]==0.0):\n",
    "            while(time[i]!=0.0):\n",
    "                i+=1\n",
    "        while(time[i]<annot[0]):\n",
    "            i+=1\n",
    "        while((time[i+win_size-1]<=annot[0]+annot[1])&(time[i+win_size-1]>annot[0])):\n",
    "            if (label(annot[2])==0):\n",
    "                sample_matrix_0.append(X[:,i:i+win_size])\n",
    "            elif (label(annot[2])==1):\n",
    "                sample_matrix_1.append(X[:,i:i+win_size])\n",
    "                \n",
    "            i+=5\n",
    "            if(i+win_size-1>=i_max):\n",
    "                break\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_matrix_0=np.array(sample_matrix_0)\n",
    "sample_matrix_1=np.array(sample_matrix_1)\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "num_0=sample_matrix_0.shape[0]\n",
    "num_1=sample_matrix_1.shape[0]\n",
    "num_window=num_0+num_1\n",
    "num_batch=int(num_window/batch_size)-2 #rest data for test\n",
    "\n",
    "for i in range(num_batch):\n",
    "    \n",
    "    #sampling training \n",
    "    random_index=range(num_0)\n",
    "    np.random.shuffle(random_index)\n",
    "    batch_0=sample_matrix_0[random_index[:int(batch_size/2)]]\n",
    "    \n",
    "    random_index=range(num_1)\n",
    "    np.random.shuffle(random_index)\n",
    "    batch_1=sample_matrix_1[random_index[:int(batch_size/2)]]\n",
    "    \n",
    "    batch=np.concatenate((batch_0,batch_1),axis=0)\n",
    "    random_index=range(batch_size)\n",
    "    np.random.shuffle(random_index)\n",
    "    batch=batch[random_index]\n",
    "    batch=np.expand_dims(batch,axis=3)\n",
    "    \n",
    "    batch_labels=[]\n",
    "    for i in random_index:\n",
    "        if i < int(batch_size/2):\n",
    "            batch_labels.append(0)\n",
    "        else :\n",
    "            batch_labels.append(1)\n",
    "\n",
    "    \n",
    "    #run\n",
    "    _,summary=sess.run([update,merged],feed_dict={labels:batch_labels,inputs:batch})\n",
    "    train_writer.add_summary(summary, i_summary)\n",
    "    i_summary+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.28121358  0.10838231]\n",
      " [-0.55150956 -0.7221539 ]\n",
      " [-0.6828587  -0.86192936]\n",
      " ...\n",
      " [-0.61721873 -0.7685868 ]\n",
      " [ 0.7588176   0.5900449 ]\n",
      " [ 0.5240012   0.7448976 ]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.57      0.55      2500\n",
      "          1       0.54      0.51      0.52      2500\n",
      "\n",
      "avg / total       0.54      0.54      0.54      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "result=[]\n",
    "labels_test=[]\n",
    "for i in range(5):\n",
    "    \n",
    "    #sampling training \n",
    "    random_index=range(num_0)\n",
    "    np.random.shuffle(random_index)\n",
    "    batch_0=sample_matrix_0[random_index[:int(batch_size/2)]]\n",
    "    \n",
    "    random_index=range(num_1)\n",
    "    np.random.shuffle(random_index)\n",
    "    batch_1=sample_matrix_1[random_index[:int(batch_size/2)]]\n",
    "    \n",
    "    batch=np.concatenate((batch_0,batch_1),axis=0)\n",
    "    random_index=range(batch_size)\n",
    "    np.random.shuffle(random_index)\n",
    "    batch=batch[random_index]\n",
    "    batch=np.expand_dims(batch,axis=3)\n",
    "    \n",
    "    batch_labels=[]\n",
    "    for i in random_index:\n",
    "        if i < int(batch_size/2):\n",
    "            batch_labels.append(0)\n",
    "        else :\n",
    "            batch_labels.append(1)\n",
    "    labels_test+=batch_labels\n",
    "    #run \n",
    "    r=sess.run([fn3],{inputs:batch})\n",
    "    result+=r\n",
    "print result[0]\n",
    "    \n",
    "result=np.concatenate(result,axis=0)\n",
    "result=np.array(result)\n",
    "result=np.argmax(result,axis=1)\n",
    "labels_test=np.array(labels_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print classification_report(labels_test,result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================================\n",
    "CASCADE learning\n",
    "=======================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'filter1:0' shape=(1, 3, 1, 8) dtype=float32_ref>\n",
      "<tf.Variable 'filter2:0' shape=(1, 3, 8, 8) dtype=float32_ref>\n",
      "<tf.Variable 'fully_connected/weights:0' shape=(1024, 20) dtype=float32_ref>\n",
      "<tf.Variable 'fully_connected/biases:0' shape=(20,) dtype=float32_ref>\n",
      "<tf.Variable 'classifier1/weights:0' shape=(20, 10) dtype=float32_ref>\n",
      "<tf.Variable 'classifier1/biases:0' shape=(10,) dtype=float32_ref>\n",
      "<tf.Variable 'classifier2/weights:0' shape=(10, 2) dtype=float32_ref>\n",
      "<tf.Variable 'classifier2/biases:0' shape=(2,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "#cascade model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "#convolution only within the dimension of timesteps (not with the channels)\n",
    "#such method may work as a smoother...\n",
    "\n",
    "batch_size=1000\n",
    "num_time_steps=20\n",
    "labels=tf.placeholder(tf.int32,shape=[1000])\n",
    "inputs=tf.placeholder(tf.float32,shape=(1000,8,num_time_steps,1))\n",
    "#with tf.name_scope(\"main/\"):\n",
    "\n",
    "#8*20*1\n",
    "filter1=tf.get_variable(name='filter1',shape=[1,3,1,8],dtype=tf.float32)\n",
    "#8*18*8\n",
    "filter2=tf.get_variable(name='filter2',shape=[1,3,8,8],dtype=tf.float32)\n",
    "#8*16*8\n",
    "\n",
    "conv1=tf.nn.conv2d(\n",
    "    input=inputs, #=cm_weighted\n",
    "    filter=filter1,\n",
    "    strides=[1,1,1,1], #[1, stride, stride, 1]\n",
    "    padding=\"VALID\",\n",
    "    use_cudnn_on_gpu=True,\n",
    "    data_format='NHWC',\n",
    ")\n",
    "\n",
    "conv1_tanh=tf.nn.tanh(conv1)\n",
    "\n",
    "conv2=tf.nn.conv2d(\n",
    "    input=conv1_tanh, #=cm_weighted\n",
    "    filter=filter2,\n",
    "    strides=[1,1,1,1], #[1, stride, stride, 1]\n",
    "    padding=\"VALID\",\n",
    "    use_cudnn_on_gpu=True,\n",
    "    data_format='NHWC',\n",
    ")\n",
    "\n",
    "conv2_tanh=tf.nn.tanh(conv2)\n",
    "conv2_flat=tf.contrib.layers.flatten(conv2_tanh)\n",
    "\n",
    "fn1=tf.contrib.layers.fully_connected(\n",
    "    inputs=conv2_flat,\n",
    "    num_outputs=20,\n",
    "    activation_fn=tf.nn.tanh,\n",
    "    normalizer_fn=None,\n",
    "    normalizer_params=None,\n",
    "    weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "    weights_regularizer=None,\n",
    "    biases_initializer=tf.zeros_initializer(),\n",
    "    biases_regularizer=None,\n",
    "    reuse=None,\n",
    "    variables_collections=None,\n",
    "    outputs_collections=None,\n",
    "    trainable=True,\n",
    "    scope=None\n",
    ")\n",
    "\n",
    "#at this level, we add a lda like a regularisation\n",
    "#LDAloss=(moy-moy)^2/(var+var)\n",
    "\n",
    "#with tf.name_scope(\"where\"):\n",
    "condition1=tf.cast(labels, tf.bool)\n",
    "condition0=tf.logical_not(condition1)\n",
    "pos1=tf.where(condition1)\n",
    "pos0=tf.where(condition0)\n",
    "input_1=tf.gather(params=fn1,indices=pos1)\n",
    "input_0=tf.gather(params=fn1,indices=pos0)\n",
    "\n",
    "mean1,var1=tf.nn.moments(input_1,axes=0)\n",
    "mean0,var0=tf.nn.moments(input_0,axes=0)\n",
    "\n",
    "ldaloss=-tf.square(tf.norm(mean1-mean0))/(tf.keras.backend.sum(var1)+tf.keras.backend.sum(var0))\n",
    "optimizer1 = tf.train.AdamOptimizer(learning_rate=0.0001,epsilon=0.001)\n",
    "gradients1=optimizer1.compute_gradients(\n",
    "    ldaloss,\n",
    "    var_list=None,\n",
    "    aggregation_method=None,\n",
    "    colocate_gradients_with_ops=False,\n",
    "    grad_loss=None\n",
    ")#return A list of (gradient, variable) pairs\n",
    "update1= optimizer1.apply_gradients(gradients1)\n",
    "#classifier\n",
    "inputs_reduce=tf.placeholder(tf.float32,shape=(1000,20))\n",
    "fn2=tf.contrib.layers.fully_connected(\n",
    "    inputs=inputs_reduce,\n",
    "    num_outputs=10,\n",
    "    activation_fn=tf.nn.tanh,\n",
    "    normalizer_fn=None,\n",
    "    normalizer_params=None,\n",
    "    weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "    weights_regularizer=None,\n",
    "    biases_initializer=tf.zeros_initializer(),\n",
    "    biases_regularizer=None,\n",
    "    reuse=None,\n",
    "    variables_collections=None,\n",
    "    outputs_collections=None,\n",
    "    trainable=True,\n",
    "    scope=\"classifier1\"\n",
    ")\n",
    "\n",
    "fn3=tf.contrib.layers.fully_connected(\n",
    "    inputs=fn2,\n",
    "    num_outputs=2,\n",
    "    activation_fn=tf.nn.tanh,\n",
    "    normalizer_fn=None,\n",
    "    normalizer_params=None,\n",
    "    weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "    weights_regularizer=None,\n",
    "    biases_initializer=tf.zeros_initializer(),\n",
    "    biases_regularizer=None,\n",
    "    reuse=None,\n",
    "    variables_collections=None,\n",
    "    outputs_collections=None,\n",
    "    trainable=True,\n",
    "    scope=\"classifier2\"\n",
    ")\n",
    "\n",
    "#loss and train\n",
    "crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=labels, logits=fn3)\n",
    "entropyloss=tf.reduce_sum(crossent)/batch_size\n",
    "# Optimization\n",
    "\n",
    "optimizer2 = tf.train.AdamOptimizer(learning_rate=0.0001,epsilon=0.001)\n",
    "gradients2=optimizer2.compute_gradients(\n",
    "    entropyloss,\n",
    "    var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"classifier\"),\n",
    "    aggregation_method=None,\n",
    "    colocate_gradients_with_ops=False,\n",
    "    grad_loss=None\n",
    ")#return A list of (gradient, variable) pairs\n",
    "update2= optimizer2.apply_gradients(gradients2)\n",
    "\n",
    "tf.summary.scalar('Loss_lda', ldaloss)\n",
    "tf.summary.scalar('Loss_ent', entropyloss)\n",
    "for i,g in enumerate(gradients1):\n",
    "    tf.summary.histogram('GGGGradients'+str(i), tf.convert_to_tensor(g[0]))\n",
    "    print tf.trainable_variables()[i]   \n",
    "for i,g in enumerate(gradients2):\n",
    "    tf.summary.histogram('GGGGradients'+str(i), tf.convert_to_tensor(g[0]))\n",
    "    print tf.trainable_variables()[i+4]\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "sess=tf.Session()\n",
    "train_writer = tf.summary.FileWriter('./summary',sess.graph)\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "    \n",
    "#debug which grandient is none\n",
    "#     for i,g in enumerate(gradients):\n",
    "#         print sess.run(g[1])\n",
    "#         if (g[0]==None):\n",
    "#             print tf.trainable_variables()[i]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_matrix_0=np.array(sample_matrix_0)\n",
    "sample_matrix_1=np.array(sample_matrix_1)\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "num_0=sample_matrix_0.shape[0]\n",
    "num_1=sample_matrix_1.shape[0]\n",
    "num_window=num_0+num_1\n",
    "num_batch=int(num_window/batch_size)-2 #rest data for test\n",
    "i_summary=0\n",
    "\n",
    "for i in range(num_batch):\n",
    "    \n",
    "    #sampling training \n",
    "    random_index=range(num_0)\n",
    "    np.random.shuffle(random_index)\n",
    "    batch_0=sample_matrix_0[random_index[:int(batch_size/2)]]\n",
    "    \n",
    "    random_index=range(num_1)\n",
    "    np.random.shuffle(random_index)\n",
    "    batch_1=sample_matrix_1[random_index[:int(batch_size/2)]]\n",
    "    \n",
    "    batch=np.concatenate((batch_0,batch_1),axis=0)\n",
    "    random_index=range(batch_size)\n",
    "    np.random.shuffle(random_index)\n",
    "    batch=batch[random_index]\n",
    "    batch=np.expand_dims(batch,axis=3)\n",
    "    \n",
    "    batch_labels=[]\n",
    "    for i in random_index:\n",
    "        if i < int(batch_size/2):\n",
    "            batch_labels.append(0)\n",
    "        else :\n",
    "            batch_labels.append(1)\n",
    "\n",
    "    \n",
    "    #run\n",
    "    sess.run([update1],feed_dict={labels:batch_labels,inputs:batch})\n",
    "\n",
    "    \n",
    "#classifier   \n",
    "sample_matrix_0_reduce=[]\n",
    "num_batch=int(sample_matrix_0.shape[0]/batch_size)\n",
    "for i in range(num_batch):\n",
    "    tmp=sess.run([fn1],feed_dict={inputs:np.expand_dims(sample_matrix_0[i*batch_size:(i+1)*batch_size],axis=3)})\n",
    "    sample_matrix_0_reduce.append(np.squeeze(np.array(tmp)))\n",
    "\n",
    "sample_matrix_1_reduce=[]\n",
    "num_batch=int(sample_matrix_1.shape[0]/batch_size)\n",
    "for i in range(num_batch):\n",
    "    tmp=sess.run([fn1],feed_dict={inputs:np.expand_dims(sample_matrix_1[i*batch_size:(i+1)*batch_size],axis=3)})\n",
    "    sample_matrix_1_reduce.append(np.squeeze(np.array(tmp)))\n",
    "\n",
    "sample_matrix_0_reduce=np.concatenate(sample_matrix_0_reduce,axis=0)\n",
    "sample_matrix_1_reduce=np.concatenate(sample_matrix_1_reduce,axis=0)\n",
    "\n",
    "\n",
    "\n",
    "num_0=sample_matrix_0_reduce.shape[0]\n",
    "num_1=sample_matrix_1_reduce.shape[0]\n",
    "num_window=num_0+num_1\n",
    "num_batch=int(num_window/batch_size)-2 #rest data for test\n",
    "\n",
    "for i in range(num_batch):\n",
    "    \n",
    "    #sampling training \n",
    "    random_index=range(num_0)\n",
    "    np.random.shuffle(random_index)\n",
    "    batch_0=sample_matrix_0_reduce[random_index[:int(batch_size/2)]]\n",
    "    \n",
    "    random_index=range(num_1)\n",
    "    np.random.shuffle(random_index)\n",
    "    batch_1=sample_matrix_1_reduce[random_index[:int(batch_size/2)]]\n",
    "    \n",
    "    batch=np.concatenate((batch_0,batch_1),axis=0)\n",
    "    random_index=range(batch_size)\n",
    "    np.random.shuffle(random_index)\n",
    "    batch=batch[random_index]\n",
    "    \n",
    "    batch_labels=[]\n",
    "    for i in random_index:\n",
    "        if i < int(batch_size/2):\n",
    "            batch_labels.append(0)\n",
    "        else :\n",
    "            batch_labels.append(1)\n",
    "\n",
    "    \n",
    "    #run\n",
    "    sess.run([update2],feed_dict={labels:batch_labels,inputs_reduce:batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.92767507 -0.7260745 ]\n",
      " [ 0.45999345  0.29836708]\n",
      " [-0.66257596 -0.5419509 ]\n",
      " ...\n",
      " [ 0.48886216  0.22159305]\n",
      " [ 0.2902739   0.07913253]\n",
      " [ 0.37447223  0.37081587]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.53      0.53      2500\n",
      "          1       0.53      0.54      0.53      2500\n",
      "\n",
      "avg / total       0.53      0.53      0.53      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "result=[]\n",
    "labels_test=[]\n",
    "for i in range(5):\n",
    "    \n",
    "    #sampling training \n",
    "    random_index=range(num_0)\n",
    "    np.random.shuffle(random_index)\n",
    "    batch_0=sample_matrix_0[random_index[:int(batch_size/2)]]\n",
    "    \n",
    "    random_index=range(num_1)\n",
    "    np.random.shuffle(random_index)\n",
    "    batch_1=sample_matrix_1[random_index[:int(batch_size/2)]]\n",
    "    \n",
    "    batch=np.concatenate((batch_0,batch_1),axis=0)\n",
    "    random_index=range(batch_size)\n",
    "    np.random.shuffle(random_index)\n",
    "    batch=batch[random_index]\n",
    "    batch=np.expand_dims(batch,axis=3)\n",
    "    \n",
    "    batch_labels=[]\n",
    "    for i in random_index:\n",
    "        if i < int(batch_size/2):\n",
    "            batch_labels.append(0)\n",
    "        else :\n",
    "            batch_labels.append(1)\n",
    "    labels_test+=batch_labels\n",
    "    #run \n",
    "    reduc=sess.run([fn1],{inputs:batch})\n",
    "    r=sess.run([fn3],{inputs_reduce:np.squeeze(reduc)})\n",
    "    result+=r\n",
    "print result[0]    \n",
    "result=np.concatenate(result,axis=0)\n",
    "result=np.array(result)\n",
    "result=np.argmax(result,axis=1)\n",
    "labels_test=np.array(labels_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print classification_report(labels_test,result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
